"""
ç½‘ç»œæŠ“å–MCPå·¥å…·é›†
æ”¯æŒHTTPè¯·æ±‚ã€ç½‘é¡µæŠ“å–ã€APIè°ƒç”¨ç­‰ç½‘ç»œç›¸å…³æ“ä½œ
éµå¾ªå…¨å±€MCPé…ç½®ï¼Œæ”¯æŒå·¥å…·é›†çº§åˆ«é…ç½®ä¼˜å…ˆçº§
"""

import asyncio
import json
import base64
import aiohttp
import aiofiles
from typing import Dict, Any, Optional, List, Union
from urllib.parse import urljoin, urlparse
from pathlib import Path
import mimetypes
from datetime import datetime

from app.core.logging import setup_logging
from app.core.secure_logging import sanitize_for_log
from app.core.mcp_tools_service import get_mcp_config_service
from app.models.mcp_config import MCPGlobalConfig
from app.tools.registry import web_scraping_tool, mcp_category

logger = setup_logging()

# Force tool discovery trigger (version 1.1)


# æ³¨å†Œç½‘ç»œæŠ“å–å·¥å…·åˆ†ç±»
@mcp_category(
    category_id="web-scraping",
    name="ç½‘ç»œæŠ“å–å·¥å…·",
    description="ç½‘é¡µæŠ“å–ã€HTTPè¯·æ±‚ã€APIè°ƒç”¨ç­‰ç½‘ç»œæ•°æ®è·å–å·¥å…·",
    icon="ğŸŒ",
    sort_order=6,
    config={
        "default_timeout": 30,
        "max_redirects": 5,
        "max_file_size_mb": 100,
        "enable_ssl_verification": True,
        "default_user_agent": "LazyAI-Studio-WebScraper/1.0",
        "rate_limit_enabled": True,
        "concurrent_requests_limit": 10,
        "enable_content_type_validation": True,
        "allowed_content_types": [
            "text/html",
            "text/plain",
            "application/json",
            "application/xml",
            "text/xml"
        ]
    }
)
def register_web_scraping_category():
    """æ³¨å†Œç½‘ç»œæŠ“å–å·¥å…·åˆ†ç±»"""
    pass


class WebScrapingConfig:
    """ç½‘ç»œæŠ“å–å·¥å…·é…ç½®ç±»"""

    def __init__(self):
        self.enabled = True
        self.user_agent = "LazyAI-Studio-WebScraper/1.0"
        self.timeout = 30
        self.max_retries = 3
        self.retry_delay = 1.0
        self.max_file_size = 100 * 1024 * 1024  # 100MB
        self.allowed_content_types = [
            "text/html", "text/plain", "application/json",
            "application/xml", "text/xml", "text/css", "text/javascript"
        ]
        self.follow_redirects = True
        self.verify_ssl = True


class WebScrapingTools:
    """ç½‘ç»œæŠ“å–å·¥å…·é›†"""

    def __init__(self):
        self.config = WebScrapingConfig()
        self.session: Optional[aiohttp.ClientSession] = None
        self.mcp_config: Optional[MCPGlobalConfig] = None
        self._load_config()

    def _load_config(self):
        """åŠ è½½é…ç½®ï¼ˆå…¨å±€MCPé…ç½® + å·¥å…·é›†é…ç½®ï¼‰"""
        try:
            config_service = get_mcp_config_service()
            self.mcp_config = config_service.get_config()

            # è·å–å·¥å…·é›†ä¸“ç”¨é…ç½®
            web_scraping_config = config_service.get_tool_category_config("web-scraping")
            custom_config = web_scraping_config.get("custom_config", {})

            # å·¥å…·é›†é…ç½®ä¼˜å…ˆçº§ > å…¨å±€é…ç½®
            if "user_agent" in custom_config:
                self.config.user_agent = custom_config["user_agent"]
            elif self.mcp_config:
                self.config.user_agent = self.mcp_config.network.user_agent

            if "timeout" in custom_config:
                self.config.timeout = custom_config["timeout"]
            elif self.mcp_config:
                self.config.timeout = self.mcp_config.network.timeout

            if "max_retries" in custom_config:
                self.config.max_retries = custom_config["max_retries"]
            elif self.mcp_config:
                self.config.max_retries = self.mcp_config.network.retry_times

            if "retry_delay" in custom_config:
                self.config.retry_delay = custom_config["retry_delay"]
            elif self.mcp_config:
                self.config.retry_delay = self.mcp_config.network.retry_delay

            if "verify_ssl" in custom_config:
                self.config.verify_ssl = custom_config["verify_ssl"]
            elif self.mcp_config:
                self.config.verify_ssl = self.mcp_config.security.verify_ssl

            logger.info(f"Loaded web scraping tools config: UA={sanitize_for_log(self.config.user_agent)}")

        except Exception as e:
            logger.warning(f"Failed to load MCP config, using defaults: {e}")

    async def _get_session(self) -> aiohttp.ClientSession:
        """è·å–HTTPä¼šè¯ï¼Œé…ç½®ä»£ç†å’Œå…¶ä»–å‚æ•°"""
        if self.session is None or self.session.closed:
            connector = aiohttp.TCPConnector(
                ssl=self.config.verify_ssl,
                limit=self.mcp_config.network.max_connections if self.mcp_config else 100
            )

            timeout = aiohttp.ClientTimeout(total=self.config.timeout)

            # è®¾ç½®ä»£ç†ï¼ˆä½¿ç”¨å…¨å±€MCPä»£ç†é…ç½®ï¼‰
            proxy_config = None
            if self.mcp_config:
                proxy_config = self.mcp_config.get_proxy_dict()

            headers = {
                "User-Agent": self.config.user_agent,
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
                "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
                "Accept-Encoding": "gzip, deflate, br",
                "DNT": "1",
                "Connection": "keep-alive",
                "Upgrade-Insecure-Requests": "1"
            }

            self.session = aiohttp.ClientSession(
                connector=connector,
                timeout=timeout,
                headers=headers
            )

            # é…ç½®ä»£ç†ï¼ˆå¦‚æœæœ‰ï¼‰
            if proxy_config:
                # aiohttpéœ€è¦ä¸ºæ¯ä¸ªè¯·æ±‚å•ç‹¬è®¾ç½®proxy
                pass

        return self.session

    def _get_proxy_url(self) -> Optional[str]:
        """è·å–ä»£ç†URL"""
        if self.mcp_config:
            proxy_dict = self.mcp_config.get_proxy_dict()
            if proxy_dict:
                return proxy_dict.get("http") or proxy_dict.get("https")
        return None

    async def _make_request_with_retry(
        self,
        method: str,
        url: str,
        **kwargs
    ) -> aiohttp.ClientResponse:
        """å¸¦é‡è¯•çš„HTTPè¯·æ±‚"""
        session = await self._get_session()
        proxy_url = self._get_proxy_url()

        for attempt in range(self.config.max_retries + 1):
            try:
                if proxy_url:
                    kwargs["proxy"] = proxy_url

                async with session.request(method, url, **kwargs) as response:
                    # æ£€æŸ¥å“åº”çŠ¶æ€
                    if response.status < 400:
                        return response
                    elif attempt < self.config.max_retries:
                        logger.warning(f"Request failed with status {response.status}, retrying ({attempt + 1}/{self.config.max_retries})")
                        await asyncio.sleep(self.config.retry_delay)
                        continue
                    else:
                        response.raise_for_status()

            except (aiohttp.ClientError, asyncio.TimeoutError) as e:
                if attempt < self.config.max_retries:
                    logger.warning(f"Request failed: {e}, retrying ({attempt + 1}/{self.config.max_retries})")
                    await asyncio.sleep(self.config.retry_delay)
                else:
                    raise e

    async def close(self):
        """å…³é—­ä¼šè¯"""
        if self.session and not self.session.closed:
            await self.session.close()


# å…¨å±€å®ä¾‹
_web_scraping_tools: Optional[WebScrapingTools] = None


def get_web_scraping_tools() -> WebScrapingTools:
    """è·å–ç½‘ç»œæŠ“å–å·¥å…·å®ä¾‹"""
    global _web_scraping_tools
    if _web_scraping_tools is None:
        _web_scraping_tools = WebScrapingTools()
    return _web_scraping_tools


def reload_web_scraping_config():
    """é‡æ–°åŠ è½½é…ç½®ï¼ˆä¾›MCPé…ç½®æœåŠ¡è°ƒç”¨ï¼‰"""
    global _web_scraping_tools
    if _web_scraping_tools is not None:
        _web_scraping_tools._load_config()


# MCPå·¥å…·å®šä¹‰
@web_scraping_tool(
    name="http_request",
    description="æ‰§è¡ŒHTTPè¯·æ±‚ï¼ˆGET, POST, PUT, DELETEç­‰ï¼‰",
    schema={
        "type": "object",
        "properties": {
            "url": {
                "type": "string",
                "description": "è¯·æ±‚URL"
            },
            "method": {
                "type": "string",
                "enum": ["GET", "POST", "PUT", "DELETE", "HEAD", "OPTIONS", "PATCH"],
                "default": "GET",
                "description": "HTTPæ–¹æ³•"
            },
            "headers": {
                "type": "object",
                "additionalProperties": {"type": "string"},
                "description": "è¯·æ±‚å¤´"
            },
            "params": {
                "type": "object",
                "description": "URLå‚æ•°"
            },
            "data": {
                "oneOf": [
                    {"type": "string"},
                    {"type": "object"}
                ],
                "description": "è¯·æ±‚ä½“æ•°æ®ï¼ˆform dataæˆ–å­—ç¬¦ä¸²ï¼‰"
            },
            "json_data": {
                "type": "object",
                "description": "JSONè¯·æ±‚ä½“"
            },
            "auth": {
                "type": "array",
                "items": {"type": "string"},
                "minItems": 2,
                "maxItems": 2,
                "description": "HTTPåŸºç¡€è®¤è¯ [username, password]"
            },
            "follow_redirects": {
                "type": "boolean",
                "description": "æ˜¯å¦è·Ÿéšé‡å®šå‘"
            }
        },
        "required": ["url"]
    },
    returns={
        "type": "object",
        "properties": {
            "status_code": {"type": "integer", "description": "HTTPçŠ¶æ€ç "},
            "headers": {"type": "object", "description": "å“åº”å¤´"},
            "content": {"type": "string", "description": "å“åº”å†…å®¹"},
            "url": {"type": "string", "description": "æœ€ç»ˆè¯·æ±‚URL"},
            "encoding": {"type": "string", "description": "å†…å®¹ç¼–ç "},
            "content_type": {"type": "string", "description": "å†…å®¹ç±»å‹"}
        }
    },
    metadata={
        "tags": ["http", "network", "request"],
        "version": "1.0.0"
    }
)
async def http_request(
    url: str,
    method: str = "GET",
    headers: Optional[Dict[str, str]] = None,
    params: Optional[Dict[str, Any]] = None,
    data: Optional[Union[str, Dict[str, Any]]] = None,
    json_data: Optional[Dict[str, Any]] = None,
    auth: Optional[tuple] = None,
    follow_redirects: Optional[bool] = None
) -> Dict[str, Any]:
    """
    æ‰§è¡ŒHTTPè¯·æ±‚

    Args:
        url: è¯·æ±‚URL
        method: HTTPæ–¹æ³•ï¼ˆGET, POST, PUT, DELETEç­‰ï¼‰
        headers: è¯·æ±‚å¤´
        params: URLå‚æ•°
        data: è¯·æ±‚ä½“æ•°æ®ï¼ˆform dataæˆ–å­—ç¬¦ä¸²ï¼‰
        json_data: JSONè¯·æ±‚ä½“
        auth: HTTPåŸºç¡€è®¤è¯ (username, password)
        follow_redirects: æ˜¯å¦è·Ÿéšé‡å®šå‘

    Returns:
        åŒ…å«å“åº”ä¿¡æ¯çš„å­—å…¸
    """
    tools = get_web_scraping_tools()

    try:
        logger.info(f"Making HTTP {method} request to {sanitize_for_log(url)}")

        # å‡†å¤‡è¯·æ±‚å‚æ•°
        kwargs = {}

        if headers:
            kwargs["headers"] = headers

        if params:
            kwargs["params"] = params

        if json_data:
            kwargs["json"] = json_data
        elif data:
            if isinstance(data, dict):
                kwargs["data"] = data
            else:
                kwargs["data"] = data

        if auth:
            kwargs["auth"] = aiohttp.BasicAuth(auth[0], auth[1])

        if follow_redirects is not None:
            kwargs["allow_redirects"] = follow_redirects
        else:
            kwargs["allow_redirects"] = tools.config.follow_redirects

        # æ‰§è¡Œè¯·æ±‚
        async with await tools._make_request_with_retry(method.upper(), url, **kwargs) as response:
            content_type = response.headers.get("Content-Type", "").lower()

            # è¯»å–å“åº”å†…å®¹
            if "application/json" in content_type:
                content = await response.json()
                content_text = json.dumps(content, ensure_ascii=False, indent=2)
            else:
                content = await response.text()
                content_text = content

            result = {
                "success": True,
                "status_code": response.status,
                "headers": dict(response.headers),
                "content": content,
                "content_text": content_text,
                "content_type": content_type,
                "url": str(response.url),
                "method": method.upper(),
                "encoding": response.get_encoding()
            }

            logger.info(f"HTTP request successful: {response.status}")
            return result

    except Exception as e:
        logger.error(f"HTTP request failed: {e}")
        return {
            "success": False,
            "error": str(e),
            "url": url,
            "method": method.upper()
        }


@web_scraping_tool(
    name="fetch_webpage",
    description="æŠ“å–ç½‘é¡µå†…å®¹ï¼Œæ”¯æŒæå–æ–‡æœ¬ã€é“¾æ¥å’Œå›¾ç‰‡ï¼Œä½¿ç”¨BeautifulSoupè§£æHTML",
    schema={
        "type": "object",
        "properties": {
            "url": {"type": "string", "description": "ç½‘é¡µURL"},
            "headers": {"type": "object", "additionalProperties": {"type": "string"}, "description": "è‡ªå®šä¹‰è¯·æ±‚å¤´"},
            "extract_text": {"type": "boolean", "description": "æ˜¯å¦æå–çº¯æ–‡æœ¬å†…å®¹", "default": True},
            "extract_links": {"type": "boolean", "description": "æ˜¯å¦æå–é“¾æ¥", "default": False},
            "extract_images": {"type": "boolean", "description": "æ˜¯å¦æå–å›¾ç‰‡URL", "default": False}
        },
        "required": ["url"]
    },
    returns={
        "type": "object",
        "properties": {
            "success": {"type": "boolean", "description": "æ˜¯å¦æˆåŠŸ"},
            "title": {"type": "string", "description": "ç½‘é¡µæ ‡é¢˜"},
            "text": {"type": "string", "description": "æå–çš„æ–‡æœ¬å†…å®¹"},
            "links": {"type": "array", "items": {"type": "object"}, "description": "æå–çš„é“¾æ¥åˆ—è¡¨"},
            "images": {"type": "array", "items": {"type": "string"}, "description": "æå–çš„å›¾ç‰‡URLåˆ—è¡¨"},
            "error": {"type": "string", "description": "é”™è¯¯ä¿¡æ¯ï¼ˆå¦‚æœå¤±è´¥ï¼‰"}
        }
    },
    metadata={"tags": ["webpage", "scraping", "html"]}
)
async def fetch_webpage(
    url: str,
    headers: Optional[Dict[str, str]] = None,
    extract_text: bool = True,
    extract_links: bool = False,
    extract_images: bool = False
) -> Dict[str, Any]:
    """
    æŠ“å–ç½‘é¡µå†…å®¹å¹¶è§£æ

    Args:
        url: ç½‘é¡µURL
        headers: è‡ªå®šä¹‰è¯·æ±‚å¤´
        extract_text: æ˜¯å¦æå–çº¯æ–‡æœ¬
        extract_links: æ˜¯å¦æå–é“¾æ¥
        extract_images: æ˜¯å¦æå–å›¾ç‰‡URL

    Returns:
        åŒ…å«ç½‘é¡µå†…å®¹å’Œè§£æç»“æœçš„å­—å…¸
    """
    tools = get_web_scraping_tools()

    try:
        logger.info(f"Fetching webpage: {sanitize_for_log(url)}")

        # æ‰§è¡ŒHTTPè¯·æ±‚
        result = await http_request(url, headers=headers)

        if not result["success"]:
            return result

        # æ£€æŸ¥å†…å®¹ç±»å‹
        content_type = result.get("content_type", "")
        if "text/html" not in content_type.lower():
            logger.warning(f"URL is not HTML content: {content_type}")
            return result

        html_content = result["content_text"]
        parsed_result = {
            "success": True,
            "url": result["url"],
            "status_code": result["status_code"],
            "headers": result["headers"],
            "html": html_content,
            "title": "",
            "text": "",
            "links": [],
            "images": []
        }

        # ç®€å•çš„HTMLè§£æï¼ˆå¯ä»¥è€ƒè™‘ä½¿ç”¨BeautifulSoupï¼‰
        try:
            from bs4 import BeautifulSoup
            soup = BeautifulSoup(html_content, 'html.parser')

            # æå–æ ‡é¢˜
            title_tag = soup.find('title')
            if title_tag:
                parsed_result["title"] = title_tag.get_text().strip()

            # æå–çº¯æ–‡æœ¬
            if extract_text:
                # ç§»é™¤scriptå’Œstyleæ ‡ç­¾
                for script in soup(["script", "style"]):
                    script.decompose()
                parsed_result["text"] = soup.get_text()

            # æå–é“¾æ¥
            if extract_links:
                links = []
                for link in soup.find_all('a', href=True):
                    href = link['href']
                    # å¤„ç†ç›¸å¯¹é“¾æ¥
                    if href.startswith('/'):
                        href = urljoin(url, href)
                    elif not href.startswith(('http://', 'https://', 'mailto:', 'tel:')):
                        href = urljoin(url, href)

                    links.append({
                        "url": href,
                        "text": link.get_text().strip(),
                        "title": link.get('title', '')
                    })
                parsed_result["links"] = links

            # æå–å›¾ç‰‡
            if extract_images:
                images = []
                for img in soup.find_all('img', src=True):
                    src = img['src']
                    # å¤„ç†ç›¸å¯¹é“¾æ¥
                    if src.startswith('/'):
                        src = urljoin(url, src)
                    elif not src.startswith(('http://', 'https://', 'data:')):
                        src = urljoin(url, src)

                    images.append({
                        "url": src,
                        "alt": img.get('alt', ''),
                        "title": img.get('title', '')
                    })
                parsed_result["images"] = images

        except ImportError:
            logger.warning("BeautifulSoup not available, skipping HTML parsing")
            # åŸºç¡€æ–‡æœ¬æå–
            if extract_text:
                import re
                # ç§»é™¤HTMLæ ‡ç­¾çš„ç®€å•æ–¹æ³•
                text = re.sub(r'<[^>]+>', '', html_content)
                parsed_result["text"] = ' '.join(text.split())

        logger.info(f"Webpage fetched successfully: {len(html_content)} characters")
        return parsed_result

    except Exception as e:
        logger.error(f"Failed to fetch webpage: {e}")
        return {
            "success": False,
            "error": str(e),
            "url": url
        }


@web_scraping_tool(
    name="download_file",
    description="ä¸‹è½½æ–‡ä»¶åˆ°æŒ‡å®šè·¯å¾„ï¼Œæ”¯æŒå¤§æ–‡ä»¶ä¸‹è½½å’Œè¿›åº¦ç›‘æ§ï¼Œå¯è®¾ç½®æ–‡ä»¶å¤§å°é™åˆ¶",
    schema={
        "type": "object",
        "properties": {
            "url": {"type": "string", "description": "æ–‡ä»¶ä¸‹è½½URL"},
            "save_path": {"type": "string", "description": "æ–‡ä»¶ä¿å­˜è·¯å¾„"},
            "headers": {"type": "object", "additionalProperties": {"type": "string"}, "description": "è‡ªå®šä¹‰è¯·æ±‚å¤´"},
            "max_size": {"type": "integer", "description": "æœ€å¤§æ–‡ä»¶å¤§å°ï¼ˆå­—èŠ‚ï¼‰"}
        },
        "required": ["url"]
    },
    returns={
        "type": "object",
        "properties": {
            "success": {"type": "boolean", "description": "æ˜¯å¦æˆåŠŸä¸‹è½½"},
            "file_path": {"type": "string", "description": "ä¸‹è½½æ–‡ä»¶çš„å®Œæ•´è·¯å¾„"},
            "file_size": {"type": "integer", "description": "æ–‡ä»¶å¤§å°ï¼ˆå­—èŠ‚ï¼‰"},
            "content_type": {"type": "string", "description": "æ–‡ä»¶å†…å®¹ç±»å‹"},
            "download_time": {"type": "number", "description": "ä¸‹è½½è€—æ—¶ï¼ˆç§’ï¼‰"},
            "error": {"type": "string", "description": "é”™è¯¯ä¿¡æ¯ï¼ˆå¦‚æœå¤±è´¥ï¼‰"}
        }
    },
    metadata={"tags": ["download", "file", "binary"]}
)
async def download_file(
    url: str,
    save_path: Optional[str] = None,
    headers: Optional[Dict[str, str]] = None,
    max_size: Optional[int] = None
) -> Dict[str, Any]:
    """
    ä¸‹è½½æ–‡ä»¶

    Args:
        url: æ–‡ä»¶URL
        save_path: ä¿å­˜è·¯å¾„ï¼ˆå¦‚æœä¸ºç©ºåˆ™è¿”å›äºŒè¿›åˆ¶å†…å®¹ï¼‰
        headers: è‡ªå®šä¹‰è¯·æ±‚å¤´
        max_size: æœ€å¤§æ–‡ä»¶å¤§å°é™åˆ¶ï¼ˆå­—èŠ‚ï¼‰

    Returns:
        åŒ…å«ä¸‹è½½ç»“æœçš„å­—å…¸
    """
    tools = get_web_scraping_tools()

    try:
        logger.info(f"Downloading file from: {sanitize_for_log(url)}")

        session = await tools._get_session()
        proxy_url = tools._get_proxy_url()

        kwargs = {}
        if headers:
            kwargs["headers"] = headers
        if proxy_url:
            kwargs["proxy"] = proxy_url

        async with session.get(url, **kwargs) as response:
            response.raise_for_status()

            # æ£€æŸ¥æ–‡ä»¶å¤§å°
            content_length = response.headers.get('Content-Length')
            file_size = int(content_length) if content_length else None
            max_allowed_size = max_size or tools.config.max_file_size

            if file_size and file_size > max_allowed_size:
                return {
                    "success": False,
                    "error": f"File too large: {file_size} bytes (max: {max_allowed_size})",
                    "url": url
                }

            # è¯»å–å†…å®¹
            content = await response.read()
            actual_size = len(content)

            if actual_size > max_allowed_size:
                return {
                    "success": False,
                    "error": f"File too large: {actual_size} bytes (max: {max_allowed_size})",
                    "url": url
                }

            result = {
                "success": True,
                "url": str(response.url),
                "status_code": response.status,
                "headers": dict(response.headers),
                "size": actual_size,
                "content_type": response.headers.get("Content-Type", ""),
                "filename": None
            }

            # ç¡®å®šæ–‡ä»¶å
            filename = None
            content_disposition = response.headers.get('Content-Disposition', '')
            if 'filename=' in content_disposition:
                filename = content_disposition.split('filename=')[1].strip('"')

            if not filename:
                # ä»URLæ¨æ–­æ–‡ä»¶å
                parsed_url = urlparse(url)
                filename = Path(parsed_url.path).name or "download"

            result["filename"] = filename

            if save_path:
                # ä¿å­˜åˆ°æ–‡ä»¶
                save_file_path = Path(save_path)
                save_file_path.parent.mkdir(parents=True, exist_ok=True)

                async with aiofiles.open(save_file_path, 'wb') as f:
                    await f.write(content)

                result["saved_path"] = str(save_file_path)
                logger.info(f"File saved to: {save_file_path}")
            else:
                # è¿”å›base64ç¼–ç çš„å†…å®¹
                result["content_base64"] = base64.b64encode(content).decode('utf-8')

            logger.info(f"File downloaded successfully: {actual_size} bytes")
            return result

    except Exception as e:
        logger.error(f"Failed to download file: {e}")
        return {
            "success": False,
            "error": str(e),
            "url": url
        }


@web_scraping_tool(
    name="api_call",
    description="æ‰§è¡ŒAPIè°ƒç”¨ï¼Œè‡ªåŠ¨å¤„ç†è®¤è¯å¤´ï¼Œæ”¯æŒJSONå“åº”è§£æå’Œé”™è¯¯å¤„ç†",
    schema={
        "type": "object",
        "properties": {
            "url": {"type": "string", "description": "APIç«¯ç‚¹URL"},
            "method": {"type": "string", "enum": ["GET", "POST", "PUT", "DELETE", "PATCH"], "description": "HTTPæ–¹æ³•", "default": "GET"},
            "headers": {"type": "object", "additionalProperties": {"type": "string"}, "description": "è¯·æ±‚å¤´å­—å…¸"},
            "params": {"type": "object", "description": "URLå‚æ•°å­—å…¸"},
            "json_data": {"type": "object", "description": "JSONè¯·æ±‚ä½“æ•°æ®"},
            "auth_token": {"type": "string", "description": "è®¤è¯ä»¤ç‰Œ"},
            "auth_type": {"type": "string", "enum": ["Bearer", "Token", "Basic"], "description": "è®¤è¯ç±»å‹", "default": "Bearer"}
        },
        "required": ["url"]
    },
    returns={
        "type": "object",
        "properties": {
            "success": {"type": "boolean", "description": "APIè°ƒç”¨æ˜¯å¦æˆåŠŸ"},
            "status_code": {"type": "integer", "description": "HTTPçŠ¶æ€ç "},
            "content": {"type": ["object", "array"], "description": "APIå“åº”å†…å®¹ï¼ˆJSONæ ¼å¼ï¼‰"},
            "headers": {"type": "object", "description": "å“åº”å¤´"},
            "response_time": {"type": "number", "description": "å“åº”æ—¶é—´ï¼ˆç§’ï¼‰"},
            "error": {"type": "string", "description": "é”™è¯¯ä¿¡æ¯ï¼ˆå¦‚æœå¤±è´¥ï¼‰"}
        }
    },
    metadata={"tags": ["api", "json", "rest"]}
)
async def api_call(
    url: str,
    method: str = "GET",
    headers: Optional[Dict[str, str]] = None,
    params: Optional[Dict[str, Any]] = None,
    json_data: Optional[Dict[str, Any]] = None,
    auth_token: Optional[str] = None,
    auth_type: str = "Bearer"
) -> Dict[str, Any]:
    """
    æ‰§è¡ŒAPIè°ƒç”¨ï¼ˆä¸“é—¨ç”¨äºRESTful APIï¼‰

    Args:
        url: APIç«¯ç‚¹URL
        method: HTTPæ–¹æ³•
        headers: è¯·æ±‚å¤´
        params: URLå‚æ•°
        json_data: JSONè¯·æ±‚ä½“
        auth_token: è®¤è¯ä»¤ç‰Œ
        auth_type: è®¤è¯ç±»å‹ï¼ˆBearer, Tokenç­‰ï¼‰

    Returns:
        APIå“åº”ç»“æœ
    """
    try:
        logger.info(f"Making API call: {method} {sanitize_for_log(url)}")

        # å‡†å¤‡è¯·æ±‚å¤´
        api_headers = {
            "Content-Type": "application/json",
            "Accept": "application/json"
        }

        if auth_token:
            api_headers["Authorization"] = f"{auth_type} {auth_token}"

        if headers:
            api_headers.update(headers)

        # æ‰§è¡Œè¯·æ±‚
        result = await http_request(
            url=url,
            method=method,
            headers=api_headers,
            params=params,
            json_data=json_data
        )

        if result["success"]:
            logger.info(f"API call successful: {result['status_code']}")
        else:
            logger.error(f"API call failed: {result.get('error', 'Unknown error')}")

        return result

    except Exception as e:
        logger.error(f"API call failed: {e}")
        return {
            "success": False,
            "error": str(e),
            "url": url,
            "method": method
        }


@web_scraping_tool(
    name="batch_requests",
    description="æ‰¹é‡æ‰§è¡ŒHTTPè¯·æ±‚ï¼Œæ”¯æŒå¹¶å‘æ§åˆ¶å’Œè¯·æ±‚é—´å»¶è¿Ÿï¼Œé€‚åˆå¤§é‡æ•°æ®æŠ“å–",
    schema={
        "type": "object",
        "properties": {
            "requests": {
                "type": "array",
                "items": {
                    "type": "object",
                    "properties": {
                        "url": {"type": "string", "description": "è¯·æ±‚URL"},
                        "method": {"type": "string", "enum": ["GET", "POST", "PUT", "DELETE"], "default": "GET"},
                        "headers": {"type": "object", "additionalProperties": {"type": "string"}},
                        "params": {"type": "object"},
                        "data": {"type": ["string", "object"]},
                        "json_data": {"type": "object"}
                    },
                    "required": ["url"]
                },
                "description": "è¯·æ±‚åˆ—è¡¨"
            },
            "max_concurrent": {"type": "integer", "minimum": 1, "maximum": 20, "description": "æœ€å¤§å¹¶å‘æ•°", "default": 5},
            "delay_between_requests": {"type": "number", "minimum": 0, "description": "è¯·æ±‚é—´å»¶è¿Ÿï¼ˆç§’ï¼‰", "default": 0.1}
        },
        "required": ["requests"]
    },
    returns={
        "type": "object",
        "properties": {
            "success": {"type": "boolean", "description": "æ‰¹é‡è¯·æ±‚æ˜¯å¦å…¨éƒ¨æˆåŠŸ"},
            "results": {
                "type": "array",
                "items": {
                    "type": "object",
                    "properties": {
                        "success": {"type": "boolean"},
                        "status_code": {"type": "integer"},
                        "content": {"type": ["string", "object"]},
                        "error": {"type": "string"}
                    }
                },
                "description": "æ¯ä¸ªè¯·æ±‚çš„ç»“æœåˆ—è¡¨"
            },
            "summary": {
                "type": "object",
                "properties": {
                    "total": {"type": "integer", "description": "æ€»è¯·æ±‚æ•°"},
                    "successful": {"type": "integer", "description": "æˆåŠŸè¯·æ±‚æ•°"},
                    "failed": {"type": "integer", "description": "å¤±è´¥è¯·æ±‚æ•°"},
                    "total_time": {"type": "number", "description": "æ€»è€—æ—¶ï¼ˆç§’ï¼‰"}
                }
            }
        }
    },
    metadata={"tags": ["batch", "concurrent", "bulk"]}
)
async def batch_requests(
    requests: List[Dict[str, Any]],
    max_concurrent: int = 5,
    delay_between_requests: float = 0.1
) -> Dict[str, Any]:
    """
    æ‰¹é‡HTTPè¯·æ±‚

    Args:
        requests: è¯·æ±‚åˆ—è¡¨ï¼Œæ¯ä¸ªè¯·æ±‚åŒ…å«urlã€methodç­‰å‚æ•°
        max_concurrent: æœ€å¤§å¹¶å‘æ•°
        delay_between_requests: è¯·æ±‚é—´å»¶è¿Ÿ

    Returns:
        æ‰¹é‡è¯·æ±‚ç»“æœ
    """
    try:
        logger.info(f"Starting batch requests: {len(requests)} requests")

        async def process_request(request_data: Dict[str, Any]) -> Dict[str, Any]:
            """å¤„ç†å•ä¸ªè¯·æ±‚"""
            try:
                if delay_between_requests > 0:
                    await asyncio.sleep(delay_between_requests)

                return await http_request(**request_data)
            except Exception as e:
                return {
                    "success": False,
                    "error": str(e),
                    "request": request_data
                }

        # ä½¿ç”¨ä¿¡å·é‡é™åˆ¶å¹¶å‘æ•°
        semaphore = asyncio.Semaphore(max_concurrent)

        async def limited_request(request_data):
            async with semaphore:
                return await process_request(request_data)

        # æ‰§è¡Œæ‰¹é‡è¯·æ±‚
        tasks = [limited_request(req) for req in requests]
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # å¤„ç†ç»“æœ
        successful = 0
        failed = 0
        processed_results = []

        for i, result in enumerate(results):
            if isinstance(result, Exception):
                processed_results.append({
                    "success": False,
                    "error": str(result),
                    "request_index": i,
                    "request": requests[i]
                })
                failed += 1
            else:
                processed_results.append({
                    **result,
                    "request_index": i
                })
                if result.get("success", False):
                    successful += 1
                else:
                    failed += 1

        batch_result = {
            "success": True,
            "total_requests": len(requests),
            "successful": successful,
            "failed": failed,
            "results": processed_results
        }

        logger.info(f"Batch requests completed: {successful} successful, {failed} failed")
        return batch_result

    except Exception as e:
        logger.error(f"Batch requests failed: {e}")
        return {
            "success": False,
            "error": str(e),
            "total_requests": len(requests)
        }


@web_scraping_tool(
    name="parse_html",
    description="è§£æHTMLå†…å®¹ï¼Œæå–æ ‡é¢˜ã€æ®µè½ã€è¡¨æ ¼ã€é“¾æ¥ç­‰ç»“æ„åŒ–æ•°æ®",
    schema={
        "type": "object",
        "properties": {
            "html_content": {"type": "string", "description": "HTMLå†…å®¹å­—ç¬¦ä¸²"},
            "url": {"type": "string", "description": "HTMLæ¥æºURLï¼ˆå¯é€‰ï¼Œç”¨äºå¤„ç†ç›¸å¯¹é“¾æ¥ï¼‰"},
            "extract_tables": {"type": "boolean", "description": "æ˜¯å¦æå–è¡¨æ ¼æ•°æ®", "default": False},
            "extract_forms": {"type": "boolean", "description": "æ˜¯å¦æå–è¡¨å•ä¿¡æ¯", "default": False},
            "extract_meta": {"type": "boolean", "description": "æ˜¯å¦æå–metaæ ‡ç­¾", "default": True},
            "css_selectors": {
                "type": "array",
                "items": {"type": "string"},
                "description": "è¦æå–çš„CSSé€‰æ‹©å™¨åˆ—è¡¨"
            },
            "remove_scripts": {"type": "boolean", "description": "æ˜¯å¦ç§»é™¤scriptæ ‡ç­¾", "default": True},
            "remove_styles": {"type": "boolean", "description": "æ˜¯å¦ç§»é™¤styleæ ‡ç­¾", "default": True}
        },
        "required": ["html_content"]
    },
    returns={
        "type": "object",
        "properties": {
            "success": {"type": "boolean", "description": "æ˜¯å¦æˆåŠŸ"},
            "title": {"type": "string", "description": "é¡µé¢æ ‡é¢˜"},
            "headings": {"type": "array", "items": {"type": "object"}, "description": "æ ‡é¢˜åˆ—è¡¨(h1-h6)"},
            "paragraphs": {"type": "array", "items": {"type": "string"}, "description": "æ®µè½æ–‡æœ¬"},
            "links": {"type": "array", "items": {"type": "object"}, "description": "é“¾æ¥åˆ—è¡¨"},
            "images": {"type": "array", "items": {"type": "object"}, "description": "å›¾ç‰‡åˆ—è¡¨"},
            "tables": {"type": "array", "items": {"type": "array"}, "description": "è¡¨æ ¼æ•°æ®"},
            "forms": {"type": "array", "items": {"type": "object"}, "description": "è¡¨å•ä¿¡æ¯"},
            "meta_tags": {"type": "object", "description": "metaæ ‡ç­¾ä¿¡æ¯"},
            "css_extracts": {"type": "object", "description": "CSSé€‰æ‹©å™¨æå–ç»“æœ"},
            "text_content": {"type": "string", "description": "çº¯æ–‡æœ¬å†…å®¹"},
            "error": {"type": "string", "description": "é”™è¯¯ä¿¡æ¯ï¼ˆå¦‚æœå¤±è´¥ï¼‰"}
        }
    },
    metadata={"tags": ["html", "parsing", "extraction", "structured-data"]}
)
async def parse_html(
    html_content: str,
    url: Optional[str] = None,
    extract_tables: bool = False,
    extract_forms: bool = False,
    extract_meta: bool = True,
    css_selectors: Optional[List[str]] = None,
    remove_scripts: bool = True,
    remove_styles: bool = True
) -> Dict[str, Any]:
    """è§£æHTMLå†…å®¹ï¼Œæå–ç»“æ„åŒ–æ•°æ®"""
    try:
        from bs4 import BeautifulSoup
        from urllib.parse import urljoin, urlparse
        import re

        # è§£æHTML
        soup = BeautifulSoup(html_content, 'html.parser')

        # ç§»é™¤ä¸éœ€è¦çš„å…ƒç´ 
        if remove_scripts:
            for script in soup.find_all('script'):
                script.decompose()
        if remove_styles:
            for style in soup.find_all('style'):
                style.decompose()

        result = {
            "success": True,
            "title": "",
            "headings": [],
            "paragraphs": [],
            "links": [],
            "images": [],
            "tables": [],
            "forms": [],
            "meta_tags": {},
            "css_extracts": {},
            "text_content": ""
        }

        # æå–æ ‡é¢˜
        title_tag = soup.find('title')
        if title_tag:
            result["title"] = title_tag.get_text().strip()

        # æå–å„çº§æ ‡é¢˜
        for i in range(1, 7):
            headings = soup.find_all(f'h{i}')
            for heading in headings:
                result["headings"].append({
                    "level": i,
                    "text": heading.get_text().strip(),
                    "id": heading.get('id'),
                    "class": heading.get('class')
                })

        # æå–æ®µè½
        paragraphs = soup.find_all('p')
        for p in paragraphs:
            text = p.get_text().strip()
            if text:
                result["paragraphs"].append(text)

        # æå–é“¾æ¥
        links = soup.find_all('a', href=True)
        for link in links:
            href = link['href']
            if url:
                href = urljoin(url, href)
            result["links"].append({
                "text": link.get_text().strip(),
                "href": href,
                "title": link.get('title'),
                "class": link.get('class')
            })

        # æå–å›¾ç‰‡
        images = soup.find_all('img')
        for img in images:
            src = img.get('src')
            if src and url:
                src = urljoin(url, src)
            result["images"].append({
                "src": src,
                "alt": img.get('alt'),
                "title": img.get('title'),
                "width": img.get('width'),
                "height": img.get('height'),
                "class": img.get('class')
            })

        # æå–è¡¨æ ¼
        if extract_tables:
            tables = soup.find_all('table')
            for table in tables:
                table_data = []
                rows = table.find_all('tr')
                for row in rows:
                    cells = row.find_all(['td', 'th'])
                    row_data = [cell.get_text().strip() for cell in cells]
                    table_data.append(row_data)
                result["tables"].append(table_data)

        # æå–è¡¨å•
        if extract_forms:
            forms = soup.find_all('form')
            for form in forms:
                inputs = form.find_all(['input', 'textarea', 'select'])
                form_data = {
                    "action": form.get('action'),
                    "method": form.get('method', 'get'),
                    "fields": []
                }
                for input_elem in inputs:
                    field = {
                        "name": input_elem.get('name'),
                        "type": input_elem.get('type'),
                        "value": input_elem.get('value'),
                        "required": input_elem.has_attr('required')
                    }
                    form_data["fields"].append(field)
                result["forms"].append(form_data)

        # æå–metaæ ‡ç­¾
        if extract_meta:
            meta_tags = soup.find_all('meta')
            for meta in meta_tags:
                name = meta.get('name') or meta.get('property') or meta.get('http-equiv')
                content = meta.get('content')
                if name and content:
                    result["meta_tags"][name] = content

        # CSSé€‰æ‹©å™¨æå–
        if css_selectors:
            for selector in css_selectors:
                try:
                    elements = soup.select(selector)
                    result["css_extracts"][selector] = [
                        {
                            "text": elem.get_text().strip(),
                            "html": str(elem),
                            "attributes": elem.attrs
                        } for elem in elements
                    ]
                except Exception as e:
                    result["css_extracts"][selector] = {"error": str(e)}

        # æå–çº¯æ–‡æœ¬
        result["text_content"] = soup.get_text().strip()

        logger.info(f"HTML parsing successful: extracted {len(result['paragraphs'])} paragraphs, {len(result['links'])} links")
        return result

    except Exception as e:
        logger.error(f"HTML parsing failed: {e}")
        return {
            "success": False,
            "error": str(e)
        }


@web_scraping_tool(
    name="parse_markdown",
    description="è§£æMarkdownå†…å®¹ï¼Œæå–æ ‡é¢˜ã€ä»£ç å—ã€è¡¨æ ¼ç­‰ç»“æ„åŒ–æ•°æ®",
    schema={
        "type": "object",
        "properties": {
            "markdown_content": {"type": "string", "description": "Markdownå†…å®¹å­—ç¬¦ä¸²"},
            "extract_code_blocks": {"type": "boolean", "description": "æ˜¯å¦æå–ä»£ç å—", "default": True},
            "extract_tables": {"type": "boolean", "description": "æ˜¯å¦æå–è¡¨æ ¼", "default": True},
            "extract_links": {"type": "boolean", "description": "æ˜¯å¦æå–é“¾æ¥", "default": True},
            "convert_to_html": {"type": "boolean", "description": "æ˜¯å¦è½¬æ¢ä¸ºHTML", "default": False}
        },
        "required": ["markdown_content"]
    },
    returns={
        "type": "object",
        "properties": {
            "success": {"type": "boolean", "description": "æ˜¯å¦æˆåŠŸ"},
            "headings": {"type": "array", "items": {"type": "object"}, "description": "æ ‡é¢˜åˆ—è¡¨"},
            "paragraphs": {"type": "array", "items": {"type": "string"}, "description": "æ®µè½æ–‡æœ¬"},
            "code_blocks": {"type": "array", "items": {"type": "object"}, "description": "ä»£ç å—"},
            "tables": {"type": "array", "items": {"type": "array"}, "description": "è¡¨æ ¼æ•°æ®"},
            "links": {"type": "array", "items": {"type": "object"}, "description": "é“¾æ¥åˆ—è¡¨"},
            "images": {"type": "array", "items": {"type": "object"}, "description": "å›¾ç‰‡åˆ—è¡¨"},
            "html_content": {"type": "string", "description": "è½¬æ¢åçš„HTMLï¼ˆå¦‚æœè¯·æ±‚ï¼‰"},
            "text_content": {"type": "string", "description": "çº¯æ–‡æœ¬å†…å®¹"},
            "error": {"type": "string", "description": "é”™è¯¯ä¿¡æ¯ï¼ˆå¦‚æœå¤±è´¥ï¼‰"}
        }
    },
    metadata={"tags": ["markdown", "parsing", "documentation", "text"]}
)
async def parse_markdown(
    markdown_content: str,
    extract_code_blocks: bool = True,
    extract_tables: bool = True,
    extract_links: bool = True,
    convert_to_html: bool = False
) -> Dict[str, Any]:
    """è§£æMarkdownå†…å®¹ï¼Œæå–ç»“æ„åŒ–æ•°æ®"""
    try:
        import re

        result = {
            "success": True,
            "headings": [],
            "paragraphs": [],
            "code_blocks": [],
            "tables": [],
            "links": [],
            "images": [],
            "html_content": "",
            "text_content": ""
        }

        lines = markdown_content.split('\n')
        current_paragraph = ""
        in_code_block = False
        current_code_block = {"language": "", "code": ""}

        for line in lines:
            # æå–æ ‡é¢˜
            if line.startswith('#'):
                level = len(line) - len(line.lstrip('#'))
                if level <= 6:
                    title_text = line.lstrip('#').strip()
                    result["headings"].append({
                        "level": level,
                        "text": title_text
                    })
                    continue

            # å¤„ç†ä»£ç å—
            if extract_code_blocks:
                if line.startswith('```'):
                    if in_code_block:
                        # ç»“æŸä»£ç å—
                        result["code_blocks"].append(current_code_block.copy())
                        current_code_block = {"language": "", "code": ""}
                        in_code_block = False
                    else:
                        # å¼€å§‹ä»£ç å—
                        language = line[3:].strip()
                        current_code_block["language"] = language
                        in_code_block = True
                    continue

                if in_code_block:
                    current_code_block["code"] += line + "\n"
                    continue

            # æå–è¡¨æ ¼
            if extract_tables and '|' in line and line.strip():
                # ç®€å•è¡¨æ ¼æ£€æµ‹
                if not hasattr(parse_markdown, '_current_table'):
                    parse_markdown._current_table = []

                cells = [cell.strip() for cell in line.split('|') if cell.strip()]
                if cells and not all(cell in ['-', ':---:', ':---', '---:'] for cell in cells):
                    parse_markdown._current_table.append(cells)
            else:
                # å®Œæˆå½“å‰è¡¨æ ¼
                if hasattr(parse_markdown, '_current_table') and parse_markdown._current_table:
                    result["tables"].append(parse_markdown._current_table)
                    delattr(parse_markdown, '_current_table')

            # æå–é“¾æ¥å’Œå›¾ç‰‡
            if extract_links:
                # é“¾æ¥ [text](url)
                link_pattern = r'\[([^\]]+)\]\(([^)]+)\)'
                links = re.findall(link_pattern, line)
                for text, url in links:
                    result["links"].append({"text": text, "href": url})

                # å›¾ç‰‡ ![alt](src)
                image_pattern = r'!\[([^\]]*)\]\(([^)]+)\)'
                images = re.findall(image_pattern, line)
                for alt, src in images:
                    result["images"].append({"alt": alt, "src": src})

            # æ”¶é›†æ®µè½
            if line.strip() and not line.startswith('#') and not in_code_block:
                current_paragraph += line + " "
            else:
                if current_paragraph.strip():
                    result["paragraphs"].append(current_paragraph.strip())
                    current_paragraph = ""

        # å¤„ç†æœ€åçš„æ®µè½
        if current_paragraph.strip():
            result["paragraphs"].append(current_paragraph.strip())

        # å®Œæˆæœ€åçš„è¡¨æ ¼
        if hasattr(parse_markdown, '_current_table') and parse_markdown._current_table:
            result["tables"].append(parse_markdown._current_table)
            delattr(parse_markdown, '_current_table')

        # è½¬æ¢ä¸ºHTMLï¼ˆå¦‚æœéœ€è¦ï¼‰
        if convert_to_html:
            try:
                import markdown
                result["html_content"] = markdown.markdown(markdown_content)
            except ImportError:
                # ç®€å•çš„Markdownåˆ°HTMLè½¬æ¢
                html_lines = []
                for line in markdown_content.split('\n'):
                    if line.startswith('#'):
                        level = len(line) - len(line.lstrip('#'))
                        text = line.lstrip('#').strip()
                        html_lines.append(f'<h{level}>{text}</h{level}>')
                    else:
                        html_lines.append(f'<p>{line}</p>')
                result["html_content"] = '\n'.join(html_lines)

        # çº¯æ–‡æœ¬å†…å®¹
        result["text_content"] = re.sub(r'[#*`_\[\]()!]', '', markdown_content)

        logger.info(f"Markdown parsing successful: {len(result['headings'])} headings, {len(result['paragraphs'])} paragraphs")
        return result

    except Exception as e:
        logger.error(f"Markdown parsing failed: {e}")
        return {
            "success": False,
            "error": str(e)
        }


@web_scraping_tool(
    name="parse_xml",
    description="è§£æXMLå†…å®¹ï¼Œæå–å…ƒç´ ã€å±æ€§ã€æ–‡æœ¬ç­‰ç»“æ„åŒ–æ•°æ®",
    schema={
        "type": "object",
        "properties": {
            "xml_content": {"type": "string", "description": "XMLå†…å®¹å­—ç¬¦ä¸²"},
            "xpath_queries": {
                "type": "array",
                "items": {"type": "string"},
                "description": "è¦æ‰§è¡Œçš„XPathæŸ¥è¯¢åˆ—è¡¨"
            },
            "extract_attributes": {"type": "boolean", "description": "æ˜¯å¦æå–å…ƒç´ å±æ€§", "default": True},
            "namespace_map": {"type": "object", "description": "XMLå‘½åç©ºé—´æ˜ å°„"},
            "parse_as_dict": {"type": "boolean", "description": "æ˜¯å¦è½¬æ¢ä¸ºå­—å…¸æ ¼å¼", "default": True}
        },
        "required": ["xml_content"]
    },
    returns={
        "type": "object",
        "properties": {
            "success": {"type": "boolean", "description": "æ˜¯å¦æˆåŠŸ"},
            "root_tag": {"type": "string", "description": "æ ¹å…ƒç´ æ ‡ç­¾"},
            "structure": {"type": "object", "description": "XMLç»“æ„æ ‘"},
            "xpath_results": {"type": "object", "description": "XPathæŸ¥è¯¢ç»“æœ"},
            "elements": {"type": "array", "items": {"type": "object"}, "description": "å…ƒç´ åˆ—è¡¨"},
            "text_content": {"type": "string", "description": "çº¯æ–‡æœ¬å†…å®¹"},
            "error": {"type": "string", "description": "é”™è¯¯ä¿¡æ¯ï¼ˆå¦‚æœå¤±è´¥ï¼‰"}
        }
    },
    metadata={"tags": ["xml", "parsing", "xpath", "structured-data"]}
)
async def parse_xml(
    xml_content: str,
    xpath_queries: Optional[List[str]] = None,
    extract_attributes: bool = True,
    namespace_map: Optional[Dict[str, str]] = None,
    parse_as_dict: bool = True
) -> Dict[str, Any]:
    """è§£æXMLå†…å®¹ï¼Œæå–ç»“æ„åŒ–æ•°æ®"""
    try:
        import xml.etree.ElementTree as ET
        from xml.dom import minidom
        import re

        result = {
            "success": True,
            "root_tag": "",
            "structure": {},
            "xpath_results": {},
            "elements": [],
            "text_content": ""
        }

        # è§£æXML
        root = ET.fromstring(xml_content)
        result["root_tag"] = root.tag

        # é€’å½’è½¬æ¢XMLä¸ºå­—å…¸
        def element_to_dict(element):
            result_dict = {}

            # æ·»åŠ å±æ€§
            if extract_attributes and element.attrib:
                result_dict["@attributes"] = element.attrib

            # æ·»åŠ æ–‡æœ¬å†…å®¹
            if element.text and element.text.strip():
                result_dict["@text"] = element.text.strip()

            # å¤„ç†å­å…ƒç´ 
            children = {}
            for child in element:
                child_dict = element_to_dict(child)
                if child.tag in children:
                    if not isinstance(children[child.tag], list):
                        children[child.tag] = [children[child.tag]]
                    children[child.tag].append(child_dict)
                else:
                    children[child.tag] = child_dict

            if children:
                result_dict.update(children)

            return result_dict

        # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
        if parse_as_dict:
            result["structure"] = {root.tag: element_to_dict(root)}

        # æå–æ‰€æœ‰å…ƒç´ ä¿¡æ¯
        for elem in root.iter():
            elem_info = {
                "tag": elem.tag,
                "text": elem.text.strip() if elem.text else "",
                "attributes": dict(elem.attrib) if extract_attributes else {},
                "tail": elem.tail.strip() if elem.tail else ""
            }
            result["elements"].append(elem_info)

        # æ‰§è¡ŒXPathæŸ¥è¯¢
        if xpath_queries:
            for query in xpath_queries:
                try:
                    elements = root.findall(query, namespace_map or {})
                    result["xpath_results"][query] = [
                        {
                            "tag": elem.tag,
                            "text": elem.text.strip() if elem.text else "",
                            "attributes": dict(elem.attrib) if extract_attributes else {}
                        } for elem in elements
                    ]
                except Exception as e:
                    result["xpath_results"][query] = {"error": str(e)}

        # æå–çº¯æ–‡æœ¬
        result["text_content"] = ET.tostring(root, method='text', encoding='unicode').strip()

        logger.info(f"XML parsing successful: {len(result['elements'])} elements extracted")
        return result

    except Exception as e:
        logger.error(f"XML parsing failed: {e}")
        return {
            "success": False,
            "error": str(e)
        }


@web_scraping_tool(
    name="parse_json",
    description="è§£æå’Œå¤„ç†JSONæ•°æ®ï¼Œæ”¯æŒJSONPathæŸ¥è¯¢å’Œæ•°æ®è½¬æ¢",
    schema={
        "type": "object",
        "properties": {
            "json_content": {"type": "string", "description": "JSONå†…å®¹å­—ç¬¦ä¸²"},
            "jsonpath_queries": {
                "type": "array",
                "items": {"type": "string"},
                "description": "JSONPathæŸ¥è¯¢è¡¨è¾¾å¼åˆ—è¡¨"
            },
            "flatten_arrays": {"type": "boolean", "description": "æ˜¯å¦å±•å¹³æ•°ç»„", "default": False},
            "extract_keys": {"type": "array", "items": {"type": "string"}, "description": "è¦æå–çš„ç‰¹å®šé”®"},
            "max_depth": {"type": "integer", "description": "æœ€å¤§è§£ææ·±åº¦", "default": 10}
        },
        "required": ["json_content"]
    },
    returns={
        "type": "object",
        "properties": {
            "success": {"type": "boolean", "description": "æ˜¯å¦æˆåŠŸ"},
            "data": {"type": "object", "description": "è§£æåçš„JSONæ•°æ®"},
            "jsonpath_results": {"type": "object", "description": "JSONPathæŸ¥è¯¢ç»“æœ"},
            "extracted_keys": {"type": "object", "description": "æå–çš„é”®å€¼å¯¹"},
            "statistics": {"type": "object", "description": "æ•°æ®ç»Ÿè®¡ä¿¡æ¯"},
            "flattened": {"type": "object", "description": "å±•å¹³åçš„æ•°æ®"},
            "error": {"type": "string", "description": "é”™è¯¯ä¿¡æ¯ï¼ˆå¦‚æœå¤±è´¥ï¼‰"}
        }
    },
    metadata={"tags": ["json", "parsing", "jsonpath", "data-processing"]}
)
async def parse_json(
    json_content: str,
    jsonpath_queries: Optional[List[str]] = None,
    flatten_arrays: bool = False,
    extract_keys: Optional[List[str]] = None,
    max_depth: int = 10
) -> Dict[str, Any]:
    """è§£æJSONæ•°æ®ï¼Œæ”¯æŒå¤æ‚æŸ¥è¯¢å’Œè½¬æ¢"""
    try:
        import json
        import re
        from collections import defaultdict

        # è§£æJSON
        data = json.loads(json_content)

        result = {
            "success": True,
            "data": data,
            "jsonpath_results": {},
            "extracted_keys": {},
            "statistics": {},
            "flattened": {}
        }

        # ç»Ÿè®¡ä¿¡æ¯
        def collect_stats(obj, depth=0):
            stats = {"total_keys": 0, "total_values": 0, "max_depth": depth, "data_types": defaultdict(int)}

            if isinstance(obj, dict):
                stats["total_keys"] += len(obj)
                for key, value in obj.items():
                    stats["data_types"][type(value).__name__] += 1
                    if isinstance(value, (dict, list)) and depth < max_depth:
                        sub_stats = collect_stats(value, depth + 1)
                        stats["total_keys"] += sub_stats["total_keys"]
                        stats["total_values"] += sub_stats["total_values"]
                        stats["max_depth"] = max(stats["max_depth"], sub_stats["max_depth"])
                        for k, v in sub_stats["data_types"].items():
                            stats["data_types"][k] += v
            elif isinstance(obj, list):
                stats["total_values"] += len(obj)
                for item in obj:
                    if isinstance(item, (dict, list)) and depth < max_depth:
                        sub_stats = collect_stats(item, depth + 1)
                        stats["total_keys"] += sub_stats["total_keys"]
                        stats["total_values"] += sub_stats["total_values"]
                        stats["max_depth"] = max(stats["max_depth"], sub_stats["max_depth"])
                        for k, v in sub_stats["data_types"].items():
                            stats["data_types"][k] += v
            else:
                stats["total_values"] += 1
                stats["data_types"][type(obj).__name__] += 1

            return stats

        result["statistics"] = collect_stats(data)
        result["statistics"]["data_types"] = dict(result["statistics"]["data_types"])

        # ç®€å•çš„JSONPathæŸ¥è¯¢å®ç°
        def simple_jsonpath(obj, path):
            """ç®€åŒ–çš„JSONPathå®ç°"""
            if not path:
                return [obj]

            if path.startswith('$.'):
                path = path[2:]

            parts = path.split('.')
            current = [obj]

            for part in parts:
                next_level = []
                for item in current:
                    if part == '*':
                        if isinstance(item, dict):
                            next_level.extend(item.values())
                        elif isinstance(item, list):
                            next_level.extend(item)
                    elif '[' in part and ']' in part:
                        # æ•°ç»„ç´¢å¼•å¤„ç†
                        key = part.split('[')[0]
                        index_str = part.split('[')[1].split(']')[0]
                        if isinstance(item, dict) and key in item:
                            if index_str == '*':
                                if isinstance(item[key], list):
                                    next_level.extend(item[key])
                            else:
                                try:
                                    index = int(index_str)
                                    if isinstance(item[key], list) and 0 <= index < len(item[key]):
                                        next_level.append(item[key][index])
                                except ValueError:
                                    pass
                    elif isinstance(item, dict) and part in item:
                        next_level.append(item[part])
                current = next_level

            return current

        # æ‰§è¡ŒJSONPathæŸ¥è¯¢
        if jsonpath_queries:
            for query in jsonpath_queries:
                try:
                    result["jsonpath_results"][query] = simple_jsonpath(data, query)
                except Exception as e:
                    result["jsonpath_results"][query] = {"error": str(e)}

        # æå–ç‰¹å®šé”®
        if extract_keys:
            def extract_recursive(obj, keys, prefix=""):
                extracted = {}
                if isinstance(obj, dict):
                    for key, value in obj.items():
                        full_key = f"{prefix}.{key}" if prefix else key
                        if key in keys:
                            extracted[full_key] = value
                        if isinstance(value, (dict, list)):
                            sub_extracted = extract_recursive(value, keys, full_key)
                            extracted.update(sub_extracted)
                elif isinstance(obj, list):
                    for i, item in enumerate(obj):
                        if isinstance(item, (dict, list)):
                            sub_extracted = extract_recursive(item, keys, f"{prefix}[{i}]")
                            extracted.update(sub_extracted)
                return extracted

            result["extracted_keys"] = extract_recursive(data, extract_keys)

        # å±•å¹³æ•°æ®
        if flatten_arrays:
            def flatten_recursive(obj, parent_key="", sep="."):
                items = []
                if isinstance(obj, dict):
                    for key, value in obj.items():
                        new_key = f"{parent_key}{sep}{key}" if parent_key else key
                        if isinstance(value, (dict, list)):
                            items.extend(flatten_recursive(value, new_key, sep).items())
                        else:
                            items.append((new_key, value))
                elif isinstance(obj, list):
                    for i, value in enumerate(obj):
                        new_key = f"{parent_key}[{i}]"
                        if isinstance(value, (dict, list)):
                            items.extend(flatten_recursive(value, new_key, sep).items())
                        else:
                            items.append((new_key, value))
                return dict(items)

            result["flattened"] = flatten_recursive(data)

        logger.info(f"JSON parsing successful: {result['statistics']['total_keys']} keys, {result['statistics']['total_values']} values")
        return result

    except json.JSONDecodeError as e:
        logger.error(f"JSON parsing failed - invalid JSON: {e}")
        return {
            "success": False,
            "error": f"Invalid JSON format: {str(e)}"
        }
    except Exception as e:
        logger.error(f"JSON processing failed: {e}")
        return {
            "success": False,
            "error": str(e)
        }


@web_scraping_tool(
    name="parse_rss",
    description="è§£æRSS/Atomè®¢é˜…æºï¼Œæå–æ–‡ç« ã€æ ‡é¢˜ã€é“¾æ¥ç­‰ä¿¡æ¯",
    schema={
        "type": "object",
        "properties": {
            "rss_content": {"type": "string", "description": "RSS/Atomå†…å®¹å­—ç¬¦ä¸²"},
            "max_items": {"type": "integer", "description": "æœ€å¤§æå–é¡¹ç›®æ•°", "default": 50},
            "extract_content": {"type": "boolean", "description": "æ˜¯å¦æå–å®Œæ•´å†…å®¹", "default": True},
            "parse_dates": {"type": "boolean", "description": "æ˜¯å¦è§£ææ—¥æœŸ", "default": True},
            "include_raw": {"type": "boolean", "description": "æ˜¯å¦åŒ…å«åŸå§‹XML", "default": False}
        },
        "required": ["rss_content"]
    },
    returns={
        "type": "object",
        "properties": {
            "success": {"type": "boolean", "description": "æ˜¯å¦æˆåŠŸ"},
            "feed_type": {"type": "string", "description": "è®¢é˜…æºç±»å‹(RSS/Atom)"},
            "feed_info": {"type": "object", "description": "è®¢é˜…æºä¿¡æ¯"},
            "items": {"type": "array", "items": {"type": "object"}, "description": "æ¡ç›®åˆ—è¡¨"},
            "statistics": {"type": "object", "description": "ç»Ÿè®¡ä¿¡æ¯"},
            "error": {"type": "string", "description": "é”™è¯¯ä¿¡æ¯ï¼ˆå¦‚æœå¤±è´¥ï¼‰"}
        }
    },
    metadata={"tags": ["rss", "atom", "feed", "news", "syndication"]}
)
async def parse_rss(
    rss_content: str,
    max_items: int = 50,
    extract_content: bool = True,
    parse_dates: bool = True,
    include_raw: bool = False
) -> Dict[str, Any]:
    """è§£æRSS/Atomè®¢é˜…æº"""
    try:
        import xml.etree.ElementTree as ET
        from datetime import datetime
        import re

        # è§£æXML
        root = ET.fromstring(rss_content)

        result = {
            "success": True,
            "feed_type": "",
            "feed_info": {},
            "items": [],
            "statistics": {}
        }

        # æ£€æµ‹è®¢é˜…æºç±»å‹
        if root.tag == 'rss' or 'rss' in root.tag.lower():
            result["feed_type"] = "RSS"
            channel = root.find('.//channel')

            # RSSè®¢é˜…æºä¿¡æ¯
            if channel is not None:
                result["feed_info"] = {
                    "title": getattr(channel.find('title'), 'text', ''),
                    "description": getattr(channel.find('description'), 'text', ''),
                    "link": getattr(channel.find('link'), 'text', ''),
                    "language": getattr(channel.find('language'), 'text', ''),
                    "lastBuildDate": getattr(channel.find('lastBuildDate'), 'text', ''),
                    "generator": getattr(channel.find('generator'), 'text', '')
                }

                # æå–RSSæ¡ç›®
                items = channel.findall('item')[:max_items]
                for item in items:
                    item_data = {
                        "title": getattr(item.find('title'), 'text', ''),
                        "description": getattr(item.find('description'), 'text', ''),
                        "link": getattr(item.find('link'), 'text', ''),
                        "pubDate": getattr(item.find('pubDate'), 'text', ''),
                        "guid": getattr(item.find('guid'), 'text', ''),
                        "author": getattr(item.find('author'), 'text', ''),
                        "categories": []
                    }

                    # æå–åˆ†ç±»
                    for category in item.findall('category'):
                        if category.text:
                            item_data["categories"].append(category.text)

                    # æå–å†…å®¹
                    if extract_content:
                        content = item.find('content:encoded', {'content': 'http://purl.org/rss/1.0/modules/content/'})
                        if content is not None:
                            item_data["content"] = content.text

                    if include_raw:
                        item_data["raw_xml"] = ET.tostring(item, encoding='unicode')

                    result["items"].append(item_data)

        elif 'atom' in root.tag.lower() or root.tag.endswith('feed'):
            result["feed_type"] = "Atom"

            # Atomè®¢é˜…æºä¿¡æ¯
            result["feed_info"] = {
                "title": getattr(root.find('.//{http://www.w3.org/2005/Atom}title'), 'text', ''),
                "subtitle": getattr(root.find('.//{http://www.w3.org/2005/Atom}subtitle'), 'text', ''),
                "id": getattr(root.find('.//{http://www.w3.org/2005/Atom}id'), 'text', ''),
                "updated": getattr(root.find('.//{http://www.w3.org/2005/Atom}updated'), 'text', ''),
                "generator": getattr(root.find('.//{http://www.w3.org/2005/Atom}generator'), 'text', '')
            }

            # æŸ¥æ‰¾é“¾æ¥
            link_elem = root.find('.//{http://www.w3.org/2005/Atom}link')
            if link_elem is not None:
                result["feed_info"]["link"] = link_elem.get('href', '')

            # æå–Atomæ¡ç›®
            entries = root.findall('.//{http://www.w3.org/2005/Atom}entry')[:max_items]
            for entry in entries:
                item_data = {
                    "title": getattr(entry.find('.//{http://www.w3.org/2005/Atom}title'), 'text', ''),
                    "summary": getattr(entry.find('.//{http://www.w3.org/2005/Atom}summary'), 'text', ''),
                    "id": getattr(entry.find('.//{http://www.w3.org/2005/Atom}id'), 'text', ''),
                    "published": getattr(entry.find('.//{http://www.w3.org/2005/Atom}published'), 'text', ''),
                    "updated": getattr(entry.find('.//{http://www.w3.org/2005/Atom}updated'), 'text', ''),
                    "categories": []
                }

                # æŸ¥æ‰¾é“¾æ¥
                link_elem = entry.find('.//{http://www.w3.org/2005/Atom}link')
                if link_elem is not None:
                    item_data["link"] = link_elem.get('href', '')

                # æŸ¥æ‰¾ä½œè€…
                author_elem = entry.find('.//{http://www.w3.org/2005/Atom}author/{http://www.w3.org/2005/Atom}name')
                if author_elem is not None:
                    item_data["author"] = author_elem.text

                # æå–åˆ†ç±»
                for category in entry.findall('.//{http://www.w3.org/2005/Atom}category'):
                    term = category.get('term')
                    if term:
                        item_data["categories"].append(term)

                # æå–å†…å®¹
                if extract_content:
                    content = entry.find('.//{http://www.w3.org/2005/Atom}content')
                    if content is not None:
                        item_data["content"] = content.text

                if include_raw:
                    item_data["raw_xml"] = ET.tostring(entry, encoding='unicode')

                result["items"].append(item_data)

        # æ—¥æœŸè§£æ
        if parse_dates:
            for item in result["items"]:
                for date_field in ['pubDate', 'published', 'updated']:
                    if date_field in item and item[date_field]:
                        try:
                            # ç®€å•æ—¥æœŸè§£æ
                            date_str = item[date_field]
                            # ç§»é™¤æ—¶åŒºä¿¡æ¯è¿›è¡Œç®€å•è§£æ
                            clean_date = re.sub(r'\s*[+-]\d{4}$', '', date_str)
                            clean_date = re.sub(r'\s*[A-Z]{3}$', '', clean_date)
                            item[f"{date_field}_parsed"] = clean_date
                        except:
                            pass

        # ç»Ÿè®¡ä¿¡æ¯
        result["statistics"] = {
            "total_items": len(result["items"]),
            "feed_type": result["feed_type"],
            "has_content": sum(1 for item in result["items"] if item.get("content")),
            "categories_count": len(set(cat for item in result["items"] for cat in item.get("categories", [])))
        }

        logger.info(f"RSS parsing successful: {result['feed_type']}, {len(result['items'])} items")
        return result

    except Exception as e:
        logger.error(f"RSS parsing failed: {e}")
        return {
            "success": False,
            "error": str(e)
        }


@web_scraping_tool(
    name="extract_css_selector",
    description="ä½¿ç”¨CSSé€‰æ‹©å™¨ä»ç½‘é¡µä¸­æå–ç‰¹å®šå†…å®¹ï¼Œæ”¯æŒå¤æ‚é€‰æ‹©å™¨å’Œæ‰¹é‡æå–",
    schema={
        "type": "object",
        "properties": {
            "url": {"type": "string", "description": "ç›®æ ‡ç½‘é¡µURL"},
            "html_content": {"type": "string", "description": "HTMLå†…å®¹å­—ç¬¦ä¸²ï¼ˆå¯é€‰ï¼Œä¼˜å…ˆä½¿ç”¨ï¼‰"},
            "css_selectors": {
                "type": "array",
                "items": {"type": "object"},
                "description": "CSSé€‰æ‹©å™¨é…ç½®åˆ—è¡¨",
                "properties": {
                    "name": {"type": "string", "description": "æå–æ•°æ®çš„åç§°"},
                    "selector": {"type": "string", "description": "CSSé€‰æ‹©å™¨"},
                    "attribute": {"type": "string", "description": "è¦æå–çš„å±æ€§ï¼ˆé»˜è®¤ä¸ºtextï¼‰"},
                    "multiple": {"type": "boolean", "description": "æ˜¯å¦æå–å¤šä¸ªå…ƒç´ ", "default": False}
                }
            },
            "wait_for_element": {"type": "string", "description": "ç­‰å¾…ç‰¹å®šå…ƒç´ å‡ºç°çš„é€‰æ‹©å™¨"},
            "headers": {"type": "object", "description": "HTTPè¯·æ±‚å¤´"}
        },
        "required": ["css_selectors"],
        "oneOf": [
            {"required": ["url"]},
            {"required": ["html_content"]}
        ]
    },
    returns={
        "type": "object",
        "properties": {
            "success": {"type": "boolean", "description": "æ˜¯å¦æˆåŠŸ"},
            "extracted_data": {"type": "object", "description": "æå–çš„æ•°æ®"},
            "selectors_used": {"type": "array", "description": "ä½¿ç”¨çš„é€‰æ‹©å™¨åˆ—è¡¨"},
            "url": {"type": "string", "description": "æºURL"},
            "statistics": {"type": "object", "description": "æå–ç»Ÿè®¡ä¿¡æ¯"},
            "error": {"type": "string", "description": "é”™è¯¯ä¿¡æ¯ï¼ˆå¦‚æœå¤±è´¥ï¼‰"}
        }
    },
    metadata={"tags": ["css", "selector", "extraction", "scraping"]}
)
async def extract_css_selector(
    css_selectors: List[Dict[str, Any]],
    url: Optional[str] = None,
    html_content: Optional[str] = None,
    wait_for_element: Optional[str] = None,
    headers: Optional[Dict[str, str]] = None
) -> Dict[str, Any]:
    """ä½¿ç”¨CSSé€‰æ‹©å™¨æå–ç½‘é¡µå†…å®¹"""
    try:
        from bs4 import BeautifulSoup
        import time

        # è·å–HTMLå†…å®¹
        if html_content:
            html = html_content
            source_url = url or "provided_html"
        elif url:
            # ä½¿ç”¨ç°æœ‰çš„http_requeståŠŸèƒ½
            request_result = await http_request(url, headers=headers or {})
            if not request_result.get("success"):
                return {
                    "success": False,
                    "error": f"Failed to fetch URL: {request_result.get('error')}"
                }
            html = request_result.get("content_text", "")
            source_url = url
        else:
            return {
                "success": False,
                "error": "Either 'url' or 'html_content' must be provided"
            }

        # è§£æHTML
        soup = BeautifulSoup(html, 'html.parser')

        result = {
            "success": True,
            "extracted_data": {},
            "selectors_used": [],
            "url": source_url,
            "statistics": {
                "total_selectors": len(css_selectors),
                "successful_extractions": 0,
                "failed_extractions": 0,
                "total_elements_found": 0
            }
        }

        # å¤„ç†æ¯ä¸ªCSSé€‰æ‹©å™¨
        for selector_config in css_selectors:
            name = selector_config.get("name", f"selector_{len(result['selectors_used'])}")
            selector = selector_config.get("selector")
            attribute = selector_config.get("attribute", "text")
            multiple = selector_config.get("multiple", False)

            if not selector:
                result["statistics"]["failed_extractions"] += 1
                result["extracted_data"][name] = {"error": "No selector provided"}
                continue

            try:
                # æŸ¥æ‰¾å…ƒç´ 
                elements = soup.select(selector)

                if not elements:
                    result["extracted_data"][name] = None if not multiple else []
                    result["selectors_used"].append({
                        "name": name,
                        "selector": selector,
                        "found": 0
                    })
                    continue

                result["statistics"]["total_elements_found"] += len(elements)

                # æå–æ•°æ®
                if multiple:
                    extracted_values = []
                    for element in elements:
                        if attribute == "text":
                            value = element.get_text().strip()
                        elif attribute == "html":
                            value = str(element)
                        else:
                            value = element.get(attribute, "")

                        # å¦‚æœéœ€è¦ï¼Œæ·»åŠ é¢å¤–çš„å±æ€§
                        if isinstance(value, str):
                            extracted_values.append(value)
                        else:
                            # æä¾›æ›´å®Œæ•´çš„å…ƒç´ ä¿¡æ¯
                            element_data = {
                                "text": element.get_text().strip(),
                                "html": str(element),
                                "attributes": dict(element.attrs),
                                "tag": element.name
                            }
                            if attribute in element_data:
                                element_data["requested_value"] = element_data[attribute]
                            extracted_values.append(element_data)

                    result["extracted_data"][name] = extracted_values
                else:
                    element = elements[0]
                    if attribute == "text":
                        result["extracted_data"][name] = element.get_text().strip()
                    elif attribute == "html":
                        result["extracted_data"][name] = str(element)
                    else:
                        result["extracted_data"][name] = element.get(attribute, "")

                result["selectors_used"].append({
                    "name": name,
                    "selector": selector,
                    "found": len(elements),
                    "multiple": multiple,
                    "attribute": attribute
                })
                result["statistics"]["successful_extractions"] += 1

            except Exception as e:
                result["statistics"]["failed_extractions"] += 1
                result["extracted_data"][name] = {"error": str(e)}
                result["selectors_used"].append({
                    "name": name,
                    "selector": selector,
                    "error": str(e)
                })

        logger.info(f"CSS extraction successful: {result['statistics']['successful_extractions']}/{result['statistics']['total_selectors']} selectors")
        return result

    except Exception as e:
        logger.error(f"CSS selector extraction failed: {e}")
        return {
            "success": False,
            "error": str(e)
        }


@web_scraping_tool(
    name="smart_extract_and_parse",
    description="æ™ºèƒ½æŠ“å–å¹¶è§£æç½‘é¡µå†…å®¹ï¼Œæ ¹æ®å†…å®¹ç±»å‹è‡ªåŠ¨é€‰æ‹©æœ€é€‚åˆçš„è§£ææ–¹å¼ï¼ˆHTML/JSON/XML/RSSï¼‰",
    schema={
        "type": "object",
        "properties": {
            "url": {"type": "string", "description": "ç›®æ ‡ç½‘é¡µURL"},
            "auto_detect_format": {"type": "boolean", "description": "æ˜¯å¦è‡ªåŠ¨æ£€æµ‹å†…å®¹æ ¼å¼", "default": True},
            "force_format": {"type": "string", "enum": ["html", "json", "xml", "rss", "markdown"], "description": "å¼ºåˆ¶æŒ‡å®šè§£ææ ¼å¼"},
            "extract_options": {
                "type": "object",
                "description": "è§£æé€‰é¡¹",
                "properties": {
                    "extract_tables": {"type": "boolean", "default": True},
                    "extract_forms": {"type": "boolean", "default": False},
                    "extract_meta": {"type": "boolean", "default": True},
                    "css_selectors": {"type": "array", "items": {"type": "string"}},
                    "max_items": {"type": "integer", "default": 50}
                }
            },
            "headers": {"type": "object", "description": "HTTPè¯·æ±‚å¤´"},
            "follow_redirects": {"type": "boolean", "description": "æ˜¯å¦è·Ÿéšé‡å®šå‘", "default": True}
        },
        "required": ["url"]
    },
    returns={
        "type": "object",
        "properties": {
            "success": {"type": "boolean", "description": "æ˜¯å¦æˆåŠŸ"},
            "detected_format": {"type": "string", "description": "æ£€æµ‹åˆ°çš„å†…å®¹æ ¼å¼"},
            "url": {"type": "string", "description": "æœ€ç»ˆURLï¼ˆå¤„ç†é‡å®šå‘åï¼‰"},
            "raw_content": {"type": "string", "description": "åŸå§‹å†…å®¹"},
            "parsed_content": {"type": "object", "description": "è§£æåçš„ç»“æ„åŒ–å†…å®¹"},
            "metadata": {"type": "object", "description": "å…ƒæ•°æ®ä¿¡æ¯"},
            "statistics": {"type": "object", "description": "ç»Ÿè®¡ä¿¡æ¯"},
            "error": {"type": "string", "description": "é”™è¯¯ä¿¡æ¯ï¼ˆå¦‚æœå¤±è´¥ï¼‰"}
        }
    },
    metadata={"tags": ["smart-parsing", "auto-detect", "multi-format", "extraction"]}
)
async def smart_extract_and_parse(
    url: str,
    auto_detect_format: bool = True,
    force_format: Optional[str] = None,
    extract_options: Optional[Dict[str, Any]] = None,
    headers: Optional[Dict[str, str]] = None,
    follow_redirects: bool = True
) -> Dict[str, Any]:
    """æ™ºèƒ½æŠ“å–å¹¶è§£æç½‘é¡µå†…å®¹"""
    try:
        import re
        from urllib.parse import urlparse

        extract_options = extract_options or {}

        # 1. æŠ“å–ç½‘é¡µå†…å®¹
        fetch_result = await http_request(url, headers=headers or {})
        if not fetch_result.get("success"):
            return {
                "success": False,
                "error": f"Failed to fetch URL: {fetch_result.get('error')}"
            }

        raw_content = fetch_result.get("content_text", "")
        final_url = fetch_result.get("url", url)
        content_type = fetch_result.get("content_type", "").lower()

        result = {
            "success": True,
            "detected_format": "",
            "url": final_url,
            "raw_content": raw_content,
            "parsed_content": {},
            "metadata": {
                "content_type": content_type,
                "content_length": len(raw_content),
                "url_domain": urlparse(final_url).netloc,
                "response_info": {
                    "status_code": fetch_result.get("status_code"),
                    "encoding": fetch_result.get("encoding")
                }
            },
            "statistics": {}
        }

        # 2. æ ¼å¼æ£€æµ‹
        detected_format = force_format

        if auto_detect_format and not force_format:
            # åŸºäºå†…å®¹ç±»å‹æ£€æµ‹
            if "application/json" in content_type:
                detected_format = "json"
            elif "application/xml" in content_type or "text/xml" in content_type:
                detected_format = "xml"
            elif "application/rss+xml" in content_type or "application/atom+xml" in content_type:
                detected_format = "rss"
            elif "text/html" in content_type:
                detected_format = "html"
            else:
                # åŸºäºå†…å®¹ç‰¹å¾æ£€æµ‹
                content_lower = raw_content.strip().lower()
                if content_lower.startswith('{') or content_lower.startswith('['):
                    detected_format = "json"
                elif content_lower.startswith('<?xml') or '<rss' in content_lower or '<feed' in content_lower:
                    if 'rss' in content_lower or 'atom' in content_lower:
                        detected_format = "rss"
                    else:
                        detected_format = "xml"
                elif content_lower.startswith('<!doctype html') or '<html' in content_lower:
                    detected_format = "html"
                elif re.match(r'^#+\s', raw_content.strip(), re.MULTILINE):
                    detected_format = "markdown"
                else:
                    detected_format = "html"  # é»˜è®¤ä½œä¸ºHTMLå¤„ç†

        result["detected_format"] = detected_format or "html"

        # 3. æ ¹æ®æ ¼å¼è§£æå†…å®¹
        if result["detected_format"] == "json":
            parse_result = await parse_json(
                raw_content,
                jsonpath_queries=extract_options.get("jsonpath_queries"),
                flatten_arrays=extract_options.get("flatten_arrays", False),
                extract_keys=extract_options.get("extract_keys"),
                max_depth=extract_options.get("max_depth", 10)
            )

        elif result["detected_format"] == "xml":
            parse_result = await parse_xml(
                raw_content,
                xpath_queries=extract_options.get("xpath_queries"),
                extract_attributes=extract_options.get("extract_attributes", True),
                namespace_map=extract_options.get("namespace_map"),
                parse_as_dict=extract_options.get("parse_as_dict", True)
            )

        elif result["detected_format"] == "rss":
            parse_result = await parse_rss(
                raw_content,
                max_items=extract_options.get("max_items", 50),
                extract_content=extract_options.get("extract_content", True),
                parse_dates=extract_options.get("parse_dates", True),
                include_raw=extract_options.get("include_raw", False)
            )

        elif result["detected_format"] == "markdown":
            parse_result = await parse_markdown(
                raw_content,
                extract_code_blocks=extract_options.get("extract_code_blocks", True),
                extract_tables=extract_options.get("extract_tables", True),
                extract_links=extract_options.get("extract_links", True),
                convert_to_html=extract_options.get("convert_to_html", False)
            )

        else:  # HTML
            parse_result = await parse_html(
                raw_content,
                url=final_url,
                extract_tables=extract_options.get("extract_tables", True),
                extract_forms=extract_options.get("extract_forms", False),
                extract_meta=extract_options.get("extract_meta", True),
                css_selectors=extract_options.get("css_selectors"),
                remove_scripts=extract_options.get("remove_scripts", True),
                remove_styles=extract_options.get("remove_styles", True)
            )

        # 4. åˆå¹¶è§£æç»“æœ
        if parse_result.get("success"):
            result["parsed_content"] = parse_result

            # æå–ç»Ÿè®¡ä¿¡æ¯
            if "statistics" in parse_result:
                result["statistics"].update(parse_result["statistics"])

            # æ ¹æ®è§£æç±»å‹æ·»åŠ ç‰¹å®šç»Ÿè®¡ä¿¡æ¯
            if result["detected_format"] == "html":
                result["statistics"]["parsing_stats"] = {
                    "paragraphs": len(parse_result.get("paragraphs", [])),
                    "links": len(parse_result.get("links", [])),
                    "images": len(parse_result.get("images", [])),
                    "headings": len(parse_result.get("headings", []))
                }
            elif result["detected_format"] == "json":
                result["statistics"]["parsing_stats"] = parse_result.get("statistics", {})
            elif result["detected_format"] in ["xml", "rss"]:
                result["statistics"]["parsing_stats"] = {
                    "elements": len(parse_result.get("elements", [])),
                    "items": len(parse_result.get("items", []))
                }

        else:
            result["parsed_content"] = {"error": parse_result.get("error", "Unknown parsing error")}
            result["statistics"]["parsing_error"] = parse_result.get("error")

        # 5. æ·»åŠ æ™ºèƒ½åˆ†æ
        result["metadata"]["analysis"] = {
            "content_type_detected": result["detected_format"],
            "parsing_successful": parse_result.get("success", False),
            "content_features": {
                "has_structured_data": result["detected_format"] in ["json", "xml", "rss"],
                "has_html_content": result["detected_format"] == "html",
                "estimated_word_count": len(raw_content.split()),
                "has_code_blocks": "```" in raw_content,
                "has_tables": "|" in raw_content or "<table" in raw_content.lower()
            }
        }

        logger.info(f"Smart extraction successful: {final_url}, format: {result['detected_format']}")
        return result

    except Exception as e:
        logger.error(f"Smart extraction failed: {e}")
        return {
            "success": False,
            "error": str(e),
            "url": url
        }


# MCPå·¥å…·æ³¨å†Œä¿¡æ¯
MCP_TOOLS = [
    {
        "name": "http_request",
        "description": "Make HTTP requests with support for various methods and configurations",
        "inputSchema": {
            "type": "object",
            "properties": {
                "url": {
                    "type": "string",
                    "description": "Request URL"
                },
                "method": {
                    "type": "string",
                    "enum": ["GET", "POST", "PUT", "DELETE", "PATCH", "HEAD", "OPTIONS"],
                    "default": "GET",
                    "description": "HTTP method"
                },
                "headers": {
                    "type": "object",
                    "description": "Request headers"
                },
                "params": {
                    "type": "object",
                    "description": "URL parameters"
                },
                "data": {
                    "oneOf": [
                        {"type": "string"},
                        {"type": "object"}
                    ],
                    "description": "Request body data"
                },
                "json_data": {
                    "type": "object",
                    "description": "JSON request body"
                },
                "auth": {
                    "type": "array",
                    "items": {"type": "string"},
                    "minItems": 2,
                    "maxItems": 2,
                    "description": "HTTP basic authentication [username, password]"
                },
                "follow_redirects": {
                    "type": "boolean",
                    "description": "Whether to follow redirects"
                }
            },
            "required": ["url"]
        }
    },
    {
        "name": "fetch_webpage",
        "description": "Fetch and parse webpage content with optional text, links, and images extraction",
        "inputSchema": {
            "type": "object",
            "properties": {
                "url": {
                    "type": "string",
                    "description": "Webpage URL"
                },
                "headers": {
                    "type": "object",
                    "description": "Custom request headers"
                },
                "extract_text": {
                    "type": "boolean",
                    "default": True,
                    "description": "Extract plain text from HTML"
                },
                "extract_links": {
                    "type": "boolean",
                    "default": False,
                    "description": "Extract all links from the page"
                },
                "extract_images": {
                    "type": "boolean",
                    "default": False,
                    "description": "Extract all image URLs from the page"
                }
            },
            "required": ["url"]
        }
    },
    {
        "name": "download_file",
        "description": "Download files from URLs with size limits and optional saving to disk",
        "inputSchema": {
            "type": "object",
            "properties": {
                "url": {
                    "type": "string",
                    "description": "File URL"
                },
                "save_path": {
                    "type": "string",
                    "description": "Local save path (optional, if not provided returns base64 content)"
                },
                "headers": {
                    "type": "object",
                    "description": "Custom request headers"
                },
                "max_size": {
                    "type": "integer",
                    "description": "Maximum file size in bytes"
                }
            },
            "required": ["url"]
        }
    },
    {
        "name": "api_call",
        "description": "Make RESTful API calls with authentication and JSON handling",
        "inputSchema": {
            "type": "object",
            "properties": {
                "url": {
                    "type": "string",
                    "description": "API endpoint URL"
                },
                "method": {
                    "type": "string",
                    "enum": ["GET", "POST", "PUT", "DELETE", "PATCH"],
                    "default": "GET",
                    "description": "HTTP method"
                },
                "headers": {
                    "type": "object",
                    "description": "Request headers"
                },
                "params": {
                    "type": "object",
                    "description": "URL parameters"
                },
                "json_data": {
                    "type": "object",
                    "description": "JSON request body"
                },
                "auth_token": {
                    "type": "string",
                    "description": "Authentication token"
                },
                "auth_type": {
                    "type": "string",
                    "default": "Bearer",
                    "description": "Authentication type (Bearer, Token, etc.)"
                }
            },
            "required": ["url"]
        }
    },
    {
        "name": "batch_requests",
        "description": "Execute multiple HTTP requests concurrently with rate limiting",
        "inputSchema": {
            "type": "object",
            "properties": {
                "requests": {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "properties": {
                            "url": {"type": "string"},
                            "method": {"type": "string"},
                            "headers": {"type": "object"},
                            "params": {"type": "object"},
                            "data": {"oneOf": [{"type": "string"}, {"type": "object"}]},
                            "json_data": {"type": "object"}
                        },
                        "required": ["url"]
                    },
                    "description": "List of requests to execute"
                },
                "max_concurrent": {
                    "type": "integer",
                    "default": 5,
                    "minimum": 1,
                    "maximum": 20,
                    "description": "Maximum concurrent requests"
                },
                "delay_between_requests": {
                    "type": "number",
                    "default": 0.1,
                    "minimum": 0,
                    "description": "Delay between requests in seconds"
                }
            },
            "required": ["requests"]
        }
    }
]