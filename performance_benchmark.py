#!/usr/bin/env python3
"""
æ€§èƒ½åŸºå‡†æµ‹è¯•è„šæœ¬
æ¯”è¾ƒä¼˜åŒ–å‰åçš„æ€§èƒ½å·®å¼‚
"""

import time
import psutil
import asyncio
from pathlib import Path
from typing import Dict, List, Any
import statistics

# å¯¼å…¥åŸå§‹æœåŠ¡
from app.core.database_service import DatabaseService, init_database_service
from app.core.yaml_service import YAMLService

# å¯¼å…¥ä¼˜åŒ–æœåŠ¡
from app.core.database_service_lite import LiteDatabaseService, init_lite_database_service
from app.core.yaml_service_optimized import OptimizedYAMLService

class PerformanceBenchmark:
    """æ€§èƒ½åŸºå‡†æµ‹è¯•"""
    
    def __init__(self):
        self.results = {}
        self.process = psutil.Process()
    
    def measure_memory_usage(self) -> float:
        """æµ‹é‡å†…å­˜ä½¿ç”¨é‡ï¼ˆMBï¼‰"""
        memory_info = self.process.memory_info()
        return memory_info.rss / 1024 / 1024
    
    def measure_cpu_usage(self) -> float:
        """æµ‹é‡CPUä½¿ç”¨ç‡"""
        return self.process.cpu_percent(interval=0.1)
    
    def benchmark_function(self, name: str, func, *args, **kwargs) -> Dict[str, Any]:
        """åŸºå‡†æµ‹è¯•å‡½æ•°"""
        print(f"\nğŸ” æµ‹è¯• {name}...")
        
        # æµ‹é‡å‰çŠ¶æ€
        start_memory = self.measure_memory_usage()
        start_time = time.time()
        
        # æ‰§è¡Œå‡½æ•°
        try:
            result = func(*args, **kwargs)
            success = True
            error = None
        except Exception as e:
            result = None
            success = False
            error = str(e)
        
        # æµ‹é‡åçŠ¶æ€
        end_time = time.time()
        end_memory = self.measure_memory_usage()
        cpu_usage = self.measure_cpu_usage()
        
        execution_time = end_time - start_time
        memory_usage = end_memory - start_memory
        
        benchmark_result = {
            'name': name,
            'success': success,
            'error': error,
            'execution_time': execution_time,
            'memory_usage': memory_usage,
            'cpu_usage': cpu_usage,
            'start_memory': start_memory,
            'end_memory': end_memory
        }
        
        print(f"  â±ï¸  æ‰§è¡Œæ—¶é—´: {execution_time:.3f}s")
        print(f"  ğŸ§  å†…å­˜ä½¿ç”¨: {memory_usage:+.2f}MB")
        print(f"  ğŸ’» CPUä½¿ç”¨: {cpu_usage:.1f}%")
        print(f"  âœ… çŠ¶æ€: {'æˆåŠŸ' if success else 'å¤±è´¥'}")
        
        if error:
            print(f"  âŒ é”™è¯¯: {error}")
        
        return benchmark_result
    
    def test_original_services(self) -> Dict[str, Any]:
        """æµ‹è¯•åŸå§‹æœåŠ¡æ€§èƒ½"""
        print("\n" + "="*60)
        print("ğŸš€ æµ‹è¯•åŸå§‹æœåŠ¡æ€§èƒ½")
        print("="*60)
        
        results = {}
        
        # æµ‹è¯•åŸå§‹æ•°æ®åº“æœåŠ¡åˆå§‹åŒ–
        def init_original_db():
            db_service = DatabaseService()
            # æ·»åŠ æ‰«æé…ç½®
            db_service.add_scan_config(
                name="models",
                path="resources/models",
                patterns=['*.yaml', '*.yml'],
                watch=True
            )
            # æ‰§è¡ŒåŒæ­¥
            db_service.sync_all()
            return db_service
        
        results['db_init'] = self.benchmark_function(
            "åŸå§‹æ•°æ®åº“æœåŠ¡åˆå§‹åŒ–", 
            init_original_db
        )
        
        # æµ‹è¯•åŸå§‹YAMLæœåŠ¡
        def load_original_models():
            return YAMLService.load_all_models()
        
        results['yaml_load'] = self.benchmark_function(
            "åŸå§‹YAMLæœåŠ¡åŠ è½½æ‰€æœ‰æ¨¡å‹",
            load_original_models
        )
        
        return results
    
    def test_optimized_services(self) -> Dict[str, Any]:
        """æµ‹è¯•ä¼˜åŒ–æœåŠ¡æ€§èƒ½"""
        print("\n" + "="*60)
        print("âš¡ æµ‹è¯•ä¼˜åŒ–æœåŠ¡æ€§èƒ½")
        print("="*60)
        
        results = {}
        
        # æµ‹è¯•ä¼˜åŒ–æ•°æ®åº“æœåŠ¡åˆå§‹åŒ–
        def init_optimized_db():
            return init_lite_database_service()
        
        results['db_init'] = self.benchmark_function(
            "ä¼˜åŒ–æ•°æ®åº“æœåŠ¡åˆå§‹åŒ–",
            init_optimized_db
        )
        
        # æµ‹è¯•ä¼˜åŒ–YAMLæœåŠ¡
        def load_optimized_models():
            yaml_service = OptimizedYAMLService()
            return yaml_service.load_all_models()
        
        results['yaml_load'] = self.benchmark_function(
            "ä¼˜åŒ–YAMLæœåŠ¡åŠ è½½æ‰€æœ‰æ¨¡å‹",
            load_optimized_models
        )
        
        # æµ‹è¯•ç¼“å­˜æ€§èƒ½
        def test_cache_performance():
            db_service = init_lite_database_service()
            # ç¬¬ä¸€æ¬¡åŠ è½½
            first_load = db_service.get_models_data()
            # ç¬¬äºŒæ¬¡åŠ è½½ï¼ˆåº”è¯¥ä½¿ç”¨ç¼“å­˜ï¼‰
            second_load = db_service.get_models_data()
            return len(first_load), len(second_load)
        
        results['cache_test'] = self.benchmark_function(
            "ç¼“å­˜æ€§èƒ½æµ‹è¯•",
            test_cache_performance
        )
        
        return results
    
    def multiple_runs_test(self, func, name: str, runs: int = 5) -> Dict[str, Any]:
        """å¤šæ¬¡è¿è¡Œæµ‹è¯•ä»¥è·å¾—æ›´å‡†ç¡®çš„ç»“æœ"""
        print(f"\nğŸ“Š å¤šæ¬¡è¿è¡Œæµ‹è¯•: {name} ({runs}æ¬¡)")
        
        execution_times = []
        memory_usages = []
        
        for i in range(runs):
            print(f"  è¿è¡Œ {i+1}/{runs}...")
            result = self.benchmark_function(f"{name}_run_{i+1}", func)
            if result['success']:
                execution_times.append(result['execution_time'])
                memory_usages.append(result['memory_usage'])
        
        if execution_times:
            return {
                'avg_execution_time': statistics.mean(execution_times),
                'min_execution_time': min(execution_times),
                'max_execution_time': max(execution_times),
                'avg_memory_usage': statistics.mean(memory_usages),
                'execution_times': execution_times,
                'memory_usages': memory_usages,
                'runs': len(execution_times)
            }
        else:
            return {'error': 'All runs failed'}
    
    def generate_comparison_report(self, original_results: Dict, optimized_results: Dict):
        """ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š"""
        print("\n" + "="*60)
        print("ğŸ“Š æ€§èƒ½å¯¹æ¯”æŠ¥å‘Š")
        print("="*60)
        
        comparisons = []
        
        for key in original_results:
            if key in optimized_results:
                orig = original_results[key]
                opt = optimized_results[key]
                
                if orig['success'] and opt['success']:
                    time_improvement = (orig['execution_time'] - opt['execution_time']) / orig['execution_time'] * 100
                    memory_improvement = (orig['memory_usage'] - opt['memory_usage']) / abs(orig['memory_usage']) * 100 if orig['memory_usage'] != 0 else 0
                    
                    comparison = {
                        'test': key,
                        'original_time': orig['execution_time'],
                        'optimized_time': opt['execution_time'],
                        'time_improvement': time_improvement,
                        'original_memory': orig['memory_usage'],
                        'optimized_memory': opt['memory_usage'],
                        'memory_improvement': memory_improvement
                    }
                    
                    comparisons.append(comparison)
                    
                    print(f"\nğŸ” {key}:")
                    print(f"  â±ï¸  æ‰§è¡Œæ—¶é—´: {orig['execution_time']:.3f}s â†’ {opt['execution_time']:.3f}s ({time_improvement:+.1f}%)")
                    print(f"  ğŸ§  å†…å­˜ä½¿ç”¨: {orig['memory_usage']:+.2f}MB â†’ {opt['memory_usage']:+.2f}MB ({memory_improvement:+.1f}%)")
        
        # æ€»ä½“æ€§èƒ½æå‡
        if comparisons:
            avg_time_improvement = statistics.mean([c['time_improvement'] for c in comparisons])
            avg_memory_improvement = statistics.mean([c['memory_improvement'] for c in comparisons])
            
            print(f"\nğŸ‰ æ€»ä½“æ€§èƒ½æå‡:")
            print(f"  âš¡ å¹³å‡æ‰§è¡Œæ—¶é—´æå‡: {avg_time_improvement:+.1f}%")
            print(f"  ğŸ’¾ å¹³å‡å†…å­˜ä½¿ç”¨æ”¹å–„: {avg_memory_improvement:+.1f}%")
        
        return comparisons
    
    def run_full_benchmark(self):
        """è¿è¡Œå®Œæ•´åŸºå‡†æµ‹è¯•"""
        print("ğŸš€ LazyAI Studio æ€§èƒ½åŸºå‡†æµ‹è¯•")
        print("="*60)
        print("æµ‹è¯•åŸå§‹æœåŠ¡ vs ä¼˜åŒ–æœåŠ¡çš„æ€§èƒ½å·®å¼‚")
        
        # æµ‹è¯•åŸå§‹æœåŠ¡
        original_results = self.test_original_services()
        
        # æµ‹è¯•ä¼˜åŒ–æœåŠ¡
        optimized_results = self.test_optimized_services()
        
        # ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
        comparisons = self.generate_comparison_report(original_results, optimized_results)
        
        # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
        import json
        report = {
            'timestamp': time.time(),
            'original_results': original_results,
            'optimized_results': optimized_results,
            'comparisons': comparisons
        }
        
        with open('performance_report.json', 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, default=str)
        
        print(f"\nğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: performance_report.json")
        
        return report


def main():
    """ä¸»å‡½æ•°"""
    benchmark = PerformanceBenchmark()
    benchmark.run_full_benchmark()


if __name__ == "__main__":
    main()